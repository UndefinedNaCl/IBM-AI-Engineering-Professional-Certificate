from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames
from ibm_watsonx_ai import Credentials
from langchain_ibm import WatsonxLLM, WatsonxEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA

import gradio as gr

# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

## LLM
def get_llm():
    model_id = 'mistralai/mixtral-8x7b-instruct-v01'
    parameters = {
        GenParams.MAX_NEW_TOKENS: 256,
        GenParams.TEMPERATURE: 0.5,
    }
    project_id = "skills-network"
    watsonx_llm = WatsonxLLM(
        model_id=model_id,
        url="https://us-south.ml.cloud.ibm.com",
        project_id=project_id,
        params=parameters,
    )
    return watsonx_llm

## Document loader
def document_loader(file):
    loader = PyPDFLoader(file.name)
    loaded_document = loader.load()
    # Extract the first 1000 characters from the content
    content_preview = ''.join([doc.page_content for doc in loaded_document])[:1000]
    print(content_preview)  # Print for the task
    return loaded_document

## Text splitter
def text_splitter(data):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=50,
        length_function=len,
    )
    chunks = text_splitter.split_documents(data)
    return chunks

## Vector db
def vector_database(chunks):
    embedding_model = watsonx_embedding()
    vectordb = Chroma.from_documents(chunks, embedding_model)
    return vectordb

## Embedding model
def watsonx_embedding():
    embed_params = {
        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,
        EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True},
    }
    watsonx_embedding = WatsonxEmbeddings(
        model_id="ibm/slate-125m-english-rtrvr",
        url="https://us-south.ml.cloud.ibm.com",
        project_id="skills-network",
        params=embed_params,
    )
    return watsonx_embedding

## Retriever
def retriever(file):
    splits = document_loader(file)
    chunks = text_splitter(splits)
    vectordb = vector_database(chunks)
    retriever = vectordb.as_retriever()
    return retriever

## QA Chain
def retriever_qa(file, query):
    llm = get_llm()
    retriever_obj = retriever(file)
    qa = RetrievalQA.from_chain_type(llm=llm, 
                                    chain_type="stuff", 
                                    retriever=retriever_obj, 
                                    return_source_documents=False)
    response = qa.invoke(query)
    return response['result']


# Create Gradio interface
rag_application = gr.Interface(
    fn=retriever_qa,
    allow_flagging="never",
    inputs=[
        gr.File(label="Upload PDF File", file_count="single", file_types=['.pdf'], type="filepath"),  # Drag and drop file upload
        gr.Textbox(label="Input Query", lines=2, placeholder="Type your question here...")
    ],
    outputs=gr.Textbox(label="Output"),
    title="RAG Chatbot",
    description="Upload a PDF document and ask any question. The chatbot will try to answer using the provided document."
)

# Launch the app
rag_application.launch(server_name="0.0.0.0", server_port= 7860)

#-----------------------------------------------#Task 1-----------------------------------------------#
# from langchain_community.document_loaders import PyPDFLoader
# import gradio as gr

# ## Document loader
# def load_first_1000_characters(file):
#     loader = PyPDFLoader(file.name)
#     loaded_document = loader.load()
#     # Extract the first 1000 characters from the content
#     content_preview = ''.join([doc.page_content for doc in loaded_document])[:1000]
#     return content_preview

# # Create a Gradio interface
# rag_application = gr.Interface(
#     fn=load_first_1000_characters,
#     allow_flagging="never",
#     inputs=gr.File(label="Upload PDF File", file_count="single", file_types=['.pdf'], type="filepath"),  # Drag and drop file upload
#     outputs=gr.Textbox(label="First 1000 Characters"),
#     title="Document Preview",
#     description="Upload a PDF document to preview the first 1000 characters."
# )

# # Launch the app
# rag_application.launch(server_name="0.0.0.0", server_port=7860)

#-----------------------------------------------#Task 2-----------------------------------------------#
# from langchain.text_splitter import RecursiveCharacterTextSplitter

# # LaTeX text to split
# latex_text = """
# \\documentclass{article}

# \\begin{document}

# \\maketitle

# \\section{Introduction}

# Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in various natural language processing tasks, including language translation, text generation, and sentiment analysis.

# \\subsection{History of LLMs}

# The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.

# \\subsection{Applications of LLMs}

# LLMs have many applications in the industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.

# \\end{document}
# """

# # Text splitter configuration
# text_splitter = RecursiveCharacterTextSplitter(
#     chunk_size=300,
#     chunk_overlap=50,
#     length_function=len,
# )

# # Splitting the LaTeX document
# chunks = text_splitter.split_text(latex_text)

# # Displaying the chunks
# for i, chunk in enumerate(chunks):
#     print(f"Chunk {i+1}:\n{chunk}\n")

#-----------------------------------------------#Task 3-----------------------------------------------#
# from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames
# from langchain_ibm import WatsonxEmbeddings

# # Watsonx embedding model configuration
# def watsonx_embedding():
#     embed_params = {
#         EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,
#         EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True},
#     }
#     watsonx_embedding_model = WatsonxEmbeddings(
#         model_id="ibm/slate-125m-english-rtrvr",  # Replace with your model ID if different
#         url="https://us-south.ml.cloud.ibm.com",
#         project_id="skills-network",
#         params=embed_params,
#     )
#     return watsonx_embedding_model

# # Sentence to embed
# query = "How are you?"

# # Generate embeddings
# embedding_model = watsonx_embedding()
# embedding_result = embedding_model.embed_query(query)

# # Display the first five embedding numbers
# print("Query:", query)
# print("First 5 embedding numbers:", embedding_result[:5])

#-----------------------------------------------#Task 4-----------------------------------------------#
# import requests
# from langchain_community.vectorstores import Chroma
# from langchain.embeddings.openai import OpenAIEmbeddings
# from langchain.text_splitter import RecursiveCharacterTextSplitter

# # Load the document from URL
# def load_document_from_url(url):
#     response = requests.get(url)
#     response.raise_for_status()  # Raise an error if the request fails
#     return response.text

# # Embed the document and create a Chroma vector database
# def create_vector_db(text):
#     # Split the document into chunks
#     text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=500,
#         chunk_overlap=50,
#         length_function=len
#     )
#     chunks = text_splitter.split_text(text)
    
#     # Create embeddings
#     embedding_model = OpenAIEmbeddings()
    
#     # Initialize Chroma database
#     vectordb = Chroma.from_texts(chunks, embedding_model)
#     return vectordb

# # Perform similarity search
# def similarity_search(vectordb, query, k=5):
#     results = vectordb.similarity_search(query, k=k)
#     return results

# # Main
# url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt"
# query = "Smoking policy"

# # Load document from the URL
# document_text = load_document_from_url(url)

# # Create vector database
# vectordb = create_vector_db(document_text)

# # Perform similarity search
# top_results = similarity_search(vectordb, query, k=5)

# # Display the results
# print("Query:", query)
# print("\nTop 5 Similarity Results:")
# for i, result in enumerate(top_results, 1):
#     print(f"Result {i}:\n{result.page_content}\n")

#-----------------------------------------------#Task 5-----------------------------------------------#
# import requests
# from langchain_community.vectorstores import Chroma
# from langchain.embeddings.openai import OpenAIEmbeddings
# from langchain.text_splitter import RecursiveCharacterTextSplitter

# # Load the document from URL
# def load_document_from_url(url):
#     response = requests.get(url)
#     response.raise_for_status()  # Raise an error if the request fails
#     return response.text

# # Create ChromaDB and configure it as a retriever
# def create_retriever(text):
#     # Split the document into chunks
#     text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=500,
#         chunk_overlap=50,
#         length_function=len
#     )
#     chunks = text_splitter.split_text(text)
    
#     # Create embeddings
#     embedding_model = OpenAIEmbeddings()
    
#     # Create ChromaDB from text chunks
#     vectordb = Chroma.from_texts(chunks, embedding_model)
    
#     # Use the database as a retriever
#     retriever = vectordb.as_retriever(search_type="similarity")
#     return retriever

# # Perform similarity search using the retriever
# def similarity_search_with_retriever(retriever, query, k=2):
#     results = retriever.get_relevant_documents(query, k=k)
#     return results

# # Main
# url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt"
# query = "Email policy"

# # Load document from the URL
# document_text = load_document_from_url(url)

# # Create retriever
# retriever = create_retriever(document_text)

# # Perform similarity search with top 2 results
# top_results = similarity_search_with_retriever(retriever, query, k=2)

# # Display the results
# print("Query:", query)
# print("\nTop 2 Similarity Results:")
# for i, result in enumerate(top_results, 1):
#     print(f"Result {i}:\n{result.page_content}\n")
